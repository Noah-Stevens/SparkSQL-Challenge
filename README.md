# Module 22 Challenge

# Home Sales Data Analysis with PySpark

## Overview

This project uses Apache Spark and PySpark SQL to analyze U.S. home sales data. It demonstrates Spark's SQL capabilities along with data optimization techniques such as caching and partitioning.

## Technologies

- Google Colab
- PySpark (Spark 3.5.5)
- Parquet
- Python 3

## Key Tasks

- Load home sales data from a CSV file
- Create and query temporary views using Spark SQL
- Analyze average home prices by features (e.g., bedrooms, bathrooms, view)
- Cache and uncache temporary tables
- Measure query performance with and without caching
- Partition the data and save as Parquet
- Read and query the partitioned Parquet data
